# Projeto Terranova - IngestÃ£o de Dados (AWS)

Este projeto migra o sistema de ingestÃ£o originalmente desenvolvido na GCP (Projeto Agronex) para a arquitetura da AWS, utilizando serviÃ§os como **AWS Lambda**, **Amazon SNS**, **Secrets Manager** e **MWAA (Apache Airflow)**.

---

## ğŸ“Œ VisÃ£o Geral da Arquitetura

- **Fonte de Dados**: APIs autenticadas via Basic Auth
- **IngestÃ£o**: AWS Lambda executando por evento ou chamada Airflow
- **PublicaÃ§Ã£o**: Mensagens formatadas sÃ£o enviadas ao **Amazon SNS**
- **OrquestraÃ§Ã£o**: Apache Airflow (MWAA) programa e aciona a ingestÃ£o
- **Segredos**: UsuÃ¡rio e senha armazenados no **AWS Secrets Manager**

---

## ğŸ“ Estrutura do Projeto

```bash
common/
â”œâ”€â”€ sns_client.py               # Publica mensagens no Amazon SNS
â”œâ”€â”€ secret_client_aws.py        # ObtÃ©m segredos do AWS Secrets Manager

terranova/
â”œâ”€â”€ carga_terranova_lambda.py   # FunÃ§Ã£o Lambda principal
â”œâ”€â”€ main_aws.py                 # Endpoint para chamada via API/Gateway ou Airflow
â”œâ”€â”€ carga_terranova_client.py   # VersÃ£o adaptada da GCP
â”œâ”€â”€ external_ingestion_aws.py   # DAG do Airflow
â”œâ”€â”€ external_ingestion_aws.yaml # ConfiguraÃ§Ãµes da DAG
```

---

## ğŸš€ ExecuÃ§Ã£o Local

1. Configure suas variÃ¡veis de ambiente:
```bash
export AWS_REGION=us-east-1
export TOPIC_ARN=arn:aws:sns:us-east-1:123456789012:topico-terranova
export URL_BASE=https://api.terranova.com/v1
```

2. Instale dependÃªncias:
```bash
pip install -r requirements.txt
```

3. Execute um ingestor manual:
```python
from carga_terranova_client import prep
prep("goodday")
```

---

## ğŸ› ï¸ Deploy como AWS Lambda

1. Comprima os arquivos com `carga_terranova_lambda.py` + pasta `common/`
2. FaÃ§a upload via AWS Console ou CLI
3. Defina a `handler`: `carga_terranova_lambda.lambda_handler`
4. Configure variÃ¡veis de ambiente:
   - `TOPIC_ARN`
   - `URL_BASE`

---

## ğŸ”„ IntegraÃ§Ã£o com Airflow (MWAA)

- Use a DAG `external_ingestion_aws.py`
- Configure a conexÃ£o HTTP: `http_lambda_terranova`
- As tasks chamam o Lambda por `SimpleHttpOperator`

---

## ğŸ” ConfiguraÃ§Ã£o de Segredos (Secrets Manager)

Crie dois segredos:
- `carga_agronex_user`
- `carga_agronex_pass`

Ambos acessados via `get_credential(secret_name)`

---

## ğŸ“ Exemplo de chamada para o Lambda

```json
{
  "option": "goodday"
}
```

---

## ğŸ“¬ Resultado esperado

Retorno JSON com status HTTP:
```json
{
  "statusCode": 200,
  "body": "{"mensagens_publicadas": 120}"
}
```

---

## ğŸ“Œ Autor

Engenheiro de Dados: Marco  
MigraÃ§Ã£o GCP â†’ AWS com foco em eficiÃªncia, automaÃ§Ã£o e boas prÃ¡ticas.



## ğŸ”„ VersÃ£o com Kinesis Firehose

Este projeto foi atualizado para usar **Amazon Kinesis Data Firehose** no lugar do Amazon SNS.

### âœ… Novo fluxo:

```
[Airflow DAG (MWAA)]
        â””â”€â”€â–º [Lambda: carga_terranova_lambda_firehose]
                   â””â”€â”€â–º [Kinesis Firehose]
                              â””â”€â”€â–º [S3 / Redshift / OpenSearch]
```

---

### ğŸ› ï¸ Deploy como AWS Lambda (Firehose)

1. Empacote `carga_terranova_lambda_firehose.py` junto com a pasta `common/`
2. FaÃ§a upload via AWS Console ou CLI
3. Defina como handler:
   ```
   carga_terranova_lambda_firehose.lambda_handler
   ```
4. Configure as variÃ¡veis de ambiente:
   - `DELIVERY_STREAM_NAME` â†’ Nome do seu stream Firehose
   - `URL_BASE`, `AWS_REGION`

---

### ğŸ“¬ Exemplo de chamada via Airflow (ou API Gateway)

```json
{
  "option": "goodday"
}
```

---

### âœ… Resultado esperado

O dado Ã© enviado para o Kinesis Firehose, que irÃ¡ armazenÃ¡-lo automaticamente no destino configurado (ex: Amazon S3, Redshift, ou OpenSearch).


---

## ğŸ“ AtualizaÃ§Ã£o dos Scripts DML

Todos os arquivos `dml_*.sql` foram adaptados para refletir a nova nomenclatura do projeto **Terranova**.

### ğŸ”„ MudanÃ§as aplicadas:
- SubstituiÃ§Ã£o de `"agronex"` por `"terranova"` em todos os arquivos DML
- Aplicado em mÃºltiplas pastas e arquivos compactados:
  - `dml_terranova.zip`
  - `dml_terranova_tasks_un.zip`

### ğŸ’¡ Uso recomendado
- Estes arquivos podem ser referenciados diretamente pelas DAGs `raw_to_bronze_terranova` e `bronze_to_silver_terranova` com `RedshiftSQLOperator`.

Certifique-se de que o caminho dos arquivos `.sql` esteja disponÃ­vel no diretÃ³rio onde o Airflow executa (`/opt/airflow/dags/queries/...` ou em um bucket S3 montado).

